The Step by Step Instructions explain how to reproduce any experiments or other activities that support the conclusions in your paper. Write this for readers who have a deep interest in your work and are studying it to improve it or compare against it. If your artifact runs for more than a few minutes, point this out, note how long it is expected to run (roughly) and explain how to run it on smaller inputs. Reviewers may choose to run on smaller inputs or larger inputs depending on available hardware.

Where appropriate, include descriptions of and links to files (included in the archive) that represent expected outputs (e.g., the log files expected to be generated by your tool on the given inputs); if there are warnings that are safe to be ignored, explain which ones they are.

The artifactâ€™s documentation should include the following: A list of claims from the paper supported by the artifact, and how/why. A list of claims from the paper not supported by the artifact, and why not.

Artifact reviewers can use this documentation to center their reviews / evaluation around these specific claims, though the reviewers will still consider whether the provided evidence is adequate to support claims that the artifact works.

There is 4 eval in total: jetstream, webi, webii, webiii.

The main pipeline for running a eval is mostly the same, with slight tweak:
0 - "./clean_log" to remove old log
1 - "python3 python/eval.py" to run the eval
2 - "python3 python/gen.py" to generate the log

below describe the tweak needed to run each eval.

note: running webi take ~3hours, and webii/webiii take ~15 hours. jetstream is ~15minutes.

result: you should roughly see that webi/webii/webiii save ~10% memory, and webii/webiii have a bigger saving then webi.
you should also see a slight time/memory saving for jetstream.
you can also click around at the web page generated to see if stuff is internally valid.
